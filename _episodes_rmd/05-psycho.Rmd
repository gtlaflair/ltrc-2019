---
title: "Part 2: CTT and CRT Test and Item Analysis"
teaching: 60
exercises: 35
questions:
  - "How do I conduct basic CTT/CRT item analyses?"
  - "How do I investigate the reliability/dependability of a test?"
  - "How do I extract indices of interest for reporting and analysis?"
objectives:
   - "Conduct classical test theory item and test analysis using `psych`."
   - "Use `rcrtan` to carry out criterion-referenced test and item analyses."
   - "Use functions from `dplyr` and `tidyr` to carry out analyses on results."
keypoints:
   - "`psych` are two packages that facilitate classical test theory analysis."
   - "`rcrtan` facilitates criterion-referenced test and item analyses."
   - "`dplyr` and `tidy` can be used to analyze the output of psychometric analysis."
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("05-")
source("../bin/download_data.R")
```

We start by loading the required packages.

```{r load-package, message=FALSE, purl=FALSE}
library(tidyverse)
library(psych)
library(CTT)
library(rcrtan)
```

If not still in the workspace, load the data we saved in the previous lesson.


```{r load-data, purl=FALSE}
test_results_1 <- read_csv('data/placement_1.csv') %>%
  mutate(country = as.factor(country)) # changes the country variable to a factor
```

## Preparing the data

Our skills with `dplyr` and `tidyr` will be useful for prepping the data for analysis.
Two of the three test analysis packages we are working require only item-level data (`psych` and `CTT`).
The third (`rcrtan`) sometimes requires a column of total test scores in addition to item-level
data.

Let's prep the data for the former two packages first.

```{r, purl=FALSE, warning=FALSE}
place_ctt <- test_results_1 %>%
  select(., (q1_list_mi:q70_read_det_an))
```

## Classical test theory analysis

Now that we have the items, we are ready to carry out some analyses. We will start with a CTT analysis
using the `psych` package. First let's take a look at what arguments our function requires. The three 
we are most concerned with are `x`, `keys`, and `delete`

- `x`: A data.frame or matrix of data, or a covariance or correlation matrix
- `keys`: If some items are to be reversed keyed, then either specify the direction of all items or just 
a vector of which items to reverse. 
- `delete`: Delete items with no variance and issue a warning

```{r, eval=FALSE, purl=FALSE}
?psych::alpha
```


```{r, purl=FALSE, warning=FALSE}
ctt_res <- psych::alpha(place_ctt, delete = FALSE) # we want to retain the items even if the indices cannot be estimated
```

The output of `psych::alpha` is a list. A list is a data structure that conatains the same or different 
types of objects. In the case of this output, the objects are different. Running `str(ctt_res)` or `glimpse(ctt_res)`
shows that there are three dataframes of different dimensions and 11 vectors of differing data types. Let's take a peek into
the first three dataframes: `total`, `alpha.drop`, and `item.stats`.


> ## Exercise
>
> With the person next to you, take a look at the help documentation for the 
> `alpha` command (hint: `?`). Read over the items listed under Value (this is
> what is returned by the command).
>
> - How would you access information about the reliability of the test?
> Which object would you extract?
> 
> > ## Solution
> >
> > ```{r reliability}
> > # 1. 
> > ctt_res$total
> > # or 2.
> > ctt_res[['total']]
> > ```
> {: .solution}
>
> - How would you find information about the item difficulty and discrimination
> parameters? Which object would you extract?
>
> > ## Solution
> > ```{r item-stats}
> > # 1. 
> > head(ctt_res$item.stats) # print the first six rows
> > # or 2.
> > head(ctt_res[['item.stats']]) # print the first six rows
> > ```
> {: .solution}
>
> - How would you find information about what the reliability of the test would
> if the items were removed from the test? Which object would you extract?
>
> > ## Solution
> > ```{r item-drop}
> > # 1. 
> > head(ctt_res$alpha.drop) # print the first six rows
> > # or 2. 
> > head(ctt_res[['alpha.drop']]) # print the first six rows
> > ```
> {: .solution}
{: .challenge}

Sometimes we want to do further analyses of the item-level data (i.e., summary of subtests or objectives). In order to do this
with the output from `psych::alpha`, we need to massage the dataframe. We can read the `separate` function below as "separate
the `question_info` column into `question`, `skill`, `objective`, and `anchor`; separate at `_`; do not remove the original 
column; if any of the four new columns have missing data, fill the rightmost column with `NA`".

```{r, purl=FALSE}
ctt_items <- ctt_res[['item.stats']] %>%
  rownames_to_column(var = 'question_info') %>% # makes the rownames a variable in the dataframe
  separate(question_info, into = c('question', 'skill', 'objective', 'anchor_status'), sep = "_", remove = FALSE, fill = 'right') %>%
  select((question_info:n), r.drop, mean) %>%# select the columns of interest
  rename('discrimination' = r.drop, 'difficulty' = mean) %>% # rename the columns
  as_tibble(.) # so it prints responsibly

ctt_items
```

One analysis we might be interested in is how the item indices differ across the two subskills (we will need our
handy `dplyr` and `tidyr` skills):

```{r, purl=FALSE}
# with wide data

skill_summary_wide <- ctt_items %>%
  select(skill, difficulty, discrimination) %>%
  group_by(skill) %>%
  summarise(n = n(),
            'Mean p' = mean(difficulty),
            'SD p' = sd(difficulty),
            'Mean d' = mean(discrimination),
            'SD d' = sd(discrimination)) %>%
  mutate_if(is.double, round, 2) # conditionally rounds all columns that are doubles to the nearest hundredth

# with long data
skill_summary_long <- ctt_items %>%
  select(skill, difficulty, discrimination) %>%
  gather(key = 'index', value, -skill) %>%
  group_by(skill, index) %>%
  summarise(n = n(),
            Mean = mean(value),
            SD = sd(value)) %>%
  mutate_if(is.double, round, 2)

# note: the kableExtra package provides a nice set of tools for creating nested tables
# for HTML and PDF documents (interesting for the results with the long data)
```

> ## Exercise
>
> Carry out the same type of summary analysis grouped by skill and objective.
> 
> > ## Solution
> >
> > ```{r objs}
> > obj_summary <- ctt_items %>%
> >   select(skill, objective, difficulty, discrimination) %>%
> >   group_by(skill, objective) %>%
> >   summarise(n = n(),
> >             'Mean p' = mean(difficulty),
> >             'SD p' = sd(difficulty),
> >             'Mean d' = mean(discrimination),
> >             'SD d' = sd(discrimination)) %>%
> >   mutate_if(is.double, round, 2)
> >
> > obj_summary
> > ```
> {: .solution}
>
> - How would you do this analysis by skill on only the anchor data? (hint: `filter`)
>
> > ## Solution
> > ```{r anchors}
> > an_summary <- ctt_items %>%
> >   filter(anchor_status == 'an') %>%
> >   select(skill, difficulty, discrimination) %>%
> >   group_by(skill) %>%
> >   summarise(n = n(),
> >             'Mean p' = mean(difficulty),
> >             'SD p' = sd(difficulty),
> >             'Mean d' = mean(discrimination),
> >             'SD d' = sd(discrimination)) %>%
> >   mutate_if(is.double, round, 2)
> >
> > an_summary
> > ```
> {: .solution}
{: .challenge}

## Criterion-referenced test analysis

In criterion-referenced test theory, the focus of the analysis is the dependability of 
classifications (e.g., master v. non-master) when evaluating the whole test and the
extent to which item indices "agree" with whole test decisions.

To evaluate the dependability of classifications, there is a function in the `rcrtan` package
called `subkoviak`. It implements Subkoviak's single administration kappa and agreement coefficients.
It requires three arguments:

- `data`: A dataframe of dichotomously scored items
- `items`: The column indices that can be used to locate the items in the dataframe
- `raw_cut_score`: The raw cut-score of the test.

We will use the `placement_1.csv` dataset for this analysis. The function returns three indices 
(`z`, `z_rounded`, `KR_est`) that were used to look up the agreement (`agree_coef.r_*`) and 
kappa (`kappa_coef.r_*`) coefficients.

```{r, purl=FALSE}
depend <- subkoviak(test_results_1, items = 5:74, raw_cut_score = 49) # cut-score = 70%

depend
```


There are also functions for carrying out CRT item analyses. The omnibus function that will return results
for a number of these analyses is `crt_iteman`. This takes similar arguments as `subkoviak` with one difference
being that the cut-score can be in raw or percent form.

```{r, purl=FALSE}
crt_res <- crt_iteman(test_results_1, items = 5:74, cut_score = 49, scale = 'raw') # cut-score = 70%

crt_res
```


> ## Exercise
>
> Carry out a summary analysis of the item indices `if_total`, `b_index`, `agree`, and `phi`.
> 
> > ## Solution
> >
> > ```{r crt-challenge}
> > crt_summary <- crt_res %>%
> > separate(items, into = c('question', 'skill', 'objective', 'anchor_status'), sep = "_", remove = FALSE, fill = 'right') %>%
> >   select(skill, if_total, b_index, agree, phi) %>%
> >   gather(key = index, value, -skill) %>%
> >   group_by(skill, index) %>%
> >   summarise(n = n(),
> >             'Mean' = mean(value),
> >             'SD' = sd(value)) %>%
> >   mutate_if(is.double, round, 2)
> >
> > crt_summary
> > ```
> {: .solution}
{: .challenge}

{% include links.md %}
